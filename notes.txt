CRAWL4AI

Adaptive crawling
Coverage: How well your collected pages cover the query terms
Consistency: Whether the information is coherent across pages
Saturation: Detecting when new pages aren't adding new information

When to Use Adaptive Crawling
Perfect For:
Research Tasks: Finding comprehensive information about a topic
Question Answering: Gathering sufficient context to answer specific queries
Knowledge Base Building: Creating focused datasets for AI/ML applications
Competitive Intelligence: Collecting complete information about specific products/features
Not Recommended For:
Full Site Archiving: When you need every page regardless of content
Structured Data Extraction: When targeting specific, known page patterns
Real-time Monitoring: When you need continuous updates

Confidence Score (0-1) 
0.0-0.3: Insufficient information, needs more crawling
0.3-0.6: Partial information, may answer basic queries
0.6-0.7: Good coverage, can answer most queries
0.7-1.0: Excellent coverage, comprehensive information


other download options:
4.1 Torch, Transformers, or All

The arun() method returns a CrawlResult object with several useful properties.


Simple vs. Deep vs. Adaptive Crawling
Simple Crawling: A basic, non-discriminatory method that traverses the web by following all available links in a pre-defined pattern (e.g., breadth-first or depth-first). It is used to index as many pages as possible for general-purpose search engines.

Deep Crawling: A more specialized method for accessing content not directly linked from public web pages, such as data behind search forms or within databases. It is used to get a more complete picture of a specific website or a set of related sites.

Adaptive Crawling: An intelligent, goal-oriented method that uses AI and machine learning to evaluate the relevance of web pages and decide which links to follow. It is highly efficient and is used to gather targeted, high-quality information on a specific topic.


Streaming vs. Non-Streaming Results
Streaming Results: Data is delivered to the user in small, continuous chunks as it becomes available. This creates an immediate, incremental display, significantly improving the perceived speed and user experience, especially for long or complex responses (e.g., AI chatbots).

Non-Streaming Results: The entire data response is generated and buffered on the server before being sent to the user in a single, complete payload. The user must wait for the entire process to finish before seeing any results. This is the traditional "request-response" model used for most standard web pages and API calls.

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),
    stream=False  # Default behavior
)



4. Filtering Content with Filter Chains
Filters help you narrow down which pages to crawl. Combine multiple filters using FilterChain for powerful targeting.


Deep crawling -> Dynamic exploration, Selective extraction
URL seeding -> Bulk processing, resource efficiency (low resource, fast)


129